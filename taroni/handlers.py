# taroni/handlers.py

import sys
import asyncio
from pathlib import Path
from google import genai
from google.genai import types
from prompt_toolkit import PromptSession
from prompt_toolkit.shortcuts import print_formatted_text, prompt
from prompt_toolkit.formatted_text import FormattedText
from prompt_toolkit.key_binding import KeyBindings
import json
import re

# utilsã‹ã‚‰ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .utils import (
    parse_and_read_files_from_prompt, add_citations,
    load_chat_history, save_chat_history, PathCompleter
)
# æ–°ã—ãä½œæˆã—ãŸãƒ„ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .tools import propose_file_modifications, ModificationPlan, FileModification

# --- æ€è€ƒè¡¨ç¤ºãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° (å¤‰æ›´ãªã—) ---
def display_thought_process(chunk, is_first_thought_ref=[True]):
    if not chunk.candidates or not hasattr(chunk.candidates[0].content, 'parts'): return
    for part in chunk.candidates[0].content.parts:
        if hasattr(part, 'thought') and part.thought and hasattr(part, 'text') and part.text:
            if is_first_thought_ref[0]: print(f"\n--- AIã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ ---", file=sys.stderr); is_first_thought_ref[0] = False
            print(part.text, end="", flush=True, file=sys.stderr)
def get_final_answer_from_chunk(chunk):
    answer_text = ""
    if chunk.candidates and hasattr(chunk.candidates[0].content, 'parts'):
         for part in chunk.candidates[0].content.parts:
             if (not hasattr(part, 'thought') or not part.thought) and hasattr(part, 'text') and part.text:
                 answer_text += part.text
    return answer_text
def reconstruct_response_from_chunks(chunks: list[types.GenerateContentResponse]) -> types.GenerateContentResponse | None:
    if not chunks: return None
    final_text = "".join(get_final_answer_from_chunk(c) for c in chunks)
    final_metadata = None
    for chunk in reversed(chunks):
        if chunk.candidates and hasattr(chunk.candidates[0], 'grounding_metadata') and chunk.candidates[0].grounding_metadata:
            final_metadata = chunk.candidates[0].grounding_metadata
            break
    final_candidate = types.Candidate(content=types.Content(parts=[types.Part(text=final_text)], role="model"), grounding_metadata=final_metadata)
    return types.GenerateContentResponse(prompt_feedback=chunks[-1].prompt_feedback if chunks else None, candidates=[final_candidate])
# --- ã“ã“ã¾ã§å¤‰æ›´ãªã— ---

# ==============================================================================
#  ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã®ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ (å¤‰æ›´ãªã—)
# ==============================================================================
async def plan_modifications(
    client: genai.Client, model_name: str, system_instruction: str | None, conversation_history: list[types.Content]
) -> tuple[ModificationPlan | None, types.Content | None]:
    agent_system_prompt = (
        "ã‚ãªãŸã¯å„ªç§€ãªAIé–‹ç™ºã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚\n"
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®é–‹ç™ºæŒ‡ç¤ºã‚„å¯¾è©±ã‚’ç†è§£ã—ã€ãã‚Œã‚’é”æˆã™ã‚‹ãŸã‚ã®å…·ä½“çš„ãªãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´è¨ˆç”»ã‚’ç«‹æ¡ˆã—ã¦ãã ã•ã„ã€‚\n"
        "è¨ˆç”»ã¯ `propose_file_modifications` é–¢æ•°ã‚’å‘¼ã³å‡ºã—ã¦ææ¡ˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n"
        "ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¿…ãšä¸¸ã”ã¨æ›¸ãæ›ãˆã‚‹å‰æã§è¨ˆç”»ã‚’ç«‹ã¦ã¦ãã ã•ã„ã€‚\n"
    )
    full_system_instruction = f"{agent_system_prompt}\n---\n\n{system_instruction}" if system_instruction else agent_system_prompt
    config = types.GenerateContentConfig(
        tools=[propose_file_modifications],
        tool_config=types.ToolConfig(function_calling_config=types.FunctionCallingConfig(mode='ANY')),
        system_instruction=full_system_instruction
    )
    print("\n[AGENT_STATUS] AIã«è¨ˆç”»ã®ç«‹æ¡ˆ/ä¿®æ­£ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆä¸­...", file=sys.stderr)
    response = await client.aio.models.generate_content(model=model_name, contents=conversation_history, config=config)
    if not response.function_calls:
        print("[AGENT_ERROR] AIãŒè¨ˆç”»ã‚’ææ¡ˆã§ãã¾ã›ã‚“ã§ã—ãŸï¼ˆFunction CallãŒã‚ã‚Šã¾ã›ã‚“ï¼‰ã€‚", file=sys.stderr)
        print("AIã®å¿œç­”:", response.text, file=sys.stderr)
        return None, None
    function_call = response.function_calls[0]
    if function_call.name != "propose_file_modifications":
        print(f"[AGENT_ERROR] AIãŒäºˆæœŸã—ãªã„é–¢æ•°({function_call.name})ã‚’å‘¼ã³å‡ºã—ã¾ã—ãŸã€‚", file=sys.stderr)
        return None, None
    try:
        plan_data = function_call.args['plan']
        modification_plan = ModificationPlan.model_validate(plan_data)
        return modification_plan, response.candidates[0].content
    except Exception as e:
        print(f"[AGENT_ERROR] AIãŒææ¡ˆã—ãŸè¨ˆç”»ã®è§£æã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", file=sys.stderr)
        print("--- AIãŒç”Ÿæˆã—ãŸç”Ÿã®è¨ˆç”»ãƒ‡ãƒ¼ã‚¿ ---", file=sys.stderr)
        print(json.dumps(function_call.args, indent=2, ensure_ascii=False), file=sys.stderr)
        print("---------------------------------", file=sys.stderr)
        return None, None
def display_plan(plan: ModificationPlan):
    print("\nâœ… AIãŒä»¥ä¸‹ã®é–‹ç™ºè¨ˆç”»ã‚’ææ¡ˆã—ã¾ã—ãŸã€‚")
    print("="*40)
    print(f"æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹: {plan.thoughts}")
    print("-"*40)
    for i, mod in enumerate(plan.modifications, 1):
        print(f"ã€ã‚¹ãƒ†ãƒƒãƒ— {i}ã€‘")
        print(f"  ãƒ•ã‚¡ã‚¤ãƒ«: {mod.file_path}")
        print(f"  ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {mod.action}")
        print(f"  å¤‰æ›´ç†ç”±: {mod.reason}")
        print(f"  å¤‰æ›´æ¦‚è¦: {mod.summary_of_changes}")
    print("="*40)
async def get_user_approval(plan: ModificationPlan) -> tuple[str, str] | None:
    display_plan(plan)
    user_input = await asyncio.to_thread(prompt, "ã“ã®è¨ˆç”»ã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n/ä¿®æ­£ç‚¹ã‚’æŒ‡ç¤º): ")
    user_input_lower = user_input.strip().lower()
    if user_input_lower in ['y', 'yes', 'ã¯ã„']: return "APPROVE", user_input
    if user_input_lower in ['n', 'no', 'ã„ã„ãˆ']: return "REJECT", user_input
    return "REPLAN", user_input
async def generate_file_content(
    client: genai.Client, model_name: str, system_instruction: str | None,
    modification: FileModification, conversation_history: list[types.Content]
) -> str:
    code_gen_system_prompt = (
        "ã‚ãªãŸã¯ãƒ—ãƒ­ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚\n"
        "ã“ã‚Œã‹ã‚‰æ¸¡ã•ã‚Œã‚‹æŒ‡ç¤ºã¨ä¼šè©±å±¥æ­´ã«åŸºã¥ãã€æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®æ–°ã—ã„å†…å®¹ã‚’**å…¨æ–‡**ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
        "ã‚ãªãŸã®å¿œç­”ã«ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹**ä»¥å¤–**ã®ä½™è¨ˆãªè§£èª¬ï¼ˆã€Œã¯ã„ã€æ‰¿çŸ¥ã—ã¾ã—ãŸã€‚ã€ã€Œä»¥ä¸‹ãŒã‚³ãƒ¼ãƒ‰ã§ã™ã€‚ã€ãªã©ï¼‰ã‚„ã€Markdownã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯è£…é£¾ï¼ˆ```python ... ```ï¼‰ã‚’**çµ¶å¯¾ã«å«ã‚ãªã„ã§ãã ã•ã„**ã€‚\n"
        "å¿œç­”ã®æœ€åˆã‹ã‚‰æœ€å¾Œã¾ã§ãŒã€ãã®ã¾ã¾ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¾ã‚Œã‚‹å†…å®¹ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚"
    )
    full_system_instruction = f"{code_gen_system_prompt}\n---\n\n{system_instruction}" if system_instruction else code_gen_system_prompt
    current_content = ""
    file_path = Path(modification.file_path)
    if modification.action == "OVERWRITE" and file_path.exists() and file_path.is_file():
        try: current_content = file_path.read_text(encoding='utf-8')
        except Exception as e: print(f"[AGENT_WARNING] ãƒ•ã‚¡ã‚¤ãƒ« '{modification.file_path}' ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", file=sys.stderr)
    generation_prompt = (
        f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ '{modification.file_path}' ã®æ–°ã—ã„å†…å®¹ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n"
        f"æ“ä½œç¨®åˆ¥: {modification.action}\n"
        f"å¤‰æ›´ç†ç”±: {modification.reason}\n"
        f"å¤‰æ›´æ¦‚è¦: {modification.summary_of_changes}\n\n"
        "ã“ã‚ŒãŒç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã§ã™ï¼ˆç©ºã®å ´åˆã¯æ–°è¦ä½œæˆã‚’æ„å‘³ã—ã¾ã™ï¼‰:\n"
        "--- CURRENT FILE CONTENT ---\n"
        f"{current_content}\n"
        "--- END OF CURRENT FILE CONTENT ---\n\n"
        "ä»¥ä¸Šã®æƒ…å ±ã¨ã“ã‚Œã¾ã§ã®ä¼šè©±å±¥æ­´ã‚’ã™ã¹ã¦è€ƒæ…®ã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã®**æ–°ã—ã„å…¨æ–‡**ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
    )
    temp_conversation = conversation_history + [types.Content(role="user", parts=[types.Part(text=generation_prompt)])]
    config = types.GenerateContentConfig(system_instruction=full_system_instruction, temperature=0.1)
    response = await client.aio.models.generate_content(model=model_name, contents=temp_conversation, config=config)
    return response.text
async def execute_plan(
    client: genai.Client, model_name: str, system_instruction: str | None,
    plan: ModificationPlan, conversation_history: list[types.Content]
):
    print("\n[AGENT_STATUS] æ‰¿èªã•ã‚ŒãŸè¨ˆç”»ã‚’å®Ÿè¡Œã—ã¾ã™...")
    successful_modifications, failed_modifications = [], []
    for i, mod in enumerate(plan.modifications, 1):
        print(f"\n--- ã‚¹ãƒ†ãƒƒãƒ— {i}/{len(plan.modifications)}: {mod.action} '{mod.file_path}' ---")
        try:
            print("[AGENT_STATUS] AIã«æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ç”Ÿæˆã•ã›ã¦ã„ã¾ã™...", file=sys.stderr)
            new_content = await generate_file_content(client, model_name, system_instruction, mod, conversation_history)
            file_path = Path(mod.file_path)
            file_path.parent.mkdir(parents=True, exist_ok=True)
            file_path.write_text(new_content, encoding='utf-8')
            print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ« '{mod.file_path}' ã®æ›´æ–°ã«æˆåŠŸã—ã¾ã—ãŸã€‚")
            successful_modifications.append(mod.file_path)
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ« '{mod.file_path}' ã®æ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
            failed_modifications.append((mod.file_path, str(e)))
    print("\n" + "="*20 + " å®Ÿè¡Œçµæœ " + "="*20)
    print("å…¨ã¦ã®è¨ˆç”»ã‚¹ãƒ†ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    if successful_modifications:
        print("\næˆåŠŸã—ãŸãƒ•ã‚¡ã‚¤ãƒ«:"); [print(f"  - {path}") for path in successful_modifications]
    if failed_modifications:
        print("\nå¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«:"); [print(f"  - {path} (ã‚¨ãƒ©ãƒ¼: {error})") for path, error in failed_modifications]
    combined_text = " ".join([m.summary_of_changes for m in plan.modifications])
    if "requirements.txt" in successful_modifications:
        print("\næ¨å¥¨ã•ã‚Œã‚‹æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        print("  - `pip install -r requirements.txt` ã‚’å®Ÿè¡Œã—ã¦ã€æ–°ã—ã„ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    if re.search(r'model|database|migrate', combined_text, re.IGNORECASE):
        print("\næ¨å¥¨ã•ã‚Œã‚‹æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:")
        print("  - `python manage.py makemigrations` ã¨ `python manage.py migrate` ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å¤‰æ›´ã‚’é©ç”¨ã—ã¦ãã ã•ã„ã€‚")
    print("="*52)
async def run_agent_session(
    client: genai.Client, model_name: str, system_instruction: str | None,
    initial_instruction: str
):
    print("\n--- AIé–‹ç™ºã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹ ---")
    processed_instruction = parse_and_read_files_from_prompt(initial_instruction)
    if not processed_instruction.strip():
        print("ã‚¨ãƒ©ãƒ¼: æŒ‡ç¤ºãŒç©ºã§ã™ã€‚", file=sys.stderr); return
    print("--- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®åˆæœŸæŒ‡ç¤º ---"); print(processed_instruction); print("---------------------------\n")
    conversation_history = [types.Content(role="user", parts=[types.Part(text=processed_instruction)])]
    try:
        while True:
            plan, ai_response_content = await plan_modifications(client, model_name, system_instruction, conversation_history)
            if not plan:
                print("[AGENT_STATUS] è¨ˆç”»ã®ç«‹æ¡ˆã«å¤±æ•—ã—ãŸãŸã‚ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¸­æ–­ã—ã¾ã™ã€‚"); return
            conversation_history.append(ai_response_content)
            action, user_feedback = await get_user_approval(plan)
            if action == "APPROVE":
                approved_plan = plan; break
            elif action == "REJECT":
                print("è¨ˆç”»ãŒå´ä¸‹ã•ã‚Œã¾ã—ãŸã€‚ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¸­æ–­ã—ã¾ã™ã€‚"); return
            elif action == "REPLAN":
                print("\nğŸ”„ è¨ˆç”»ã®ä¿®æ­£ã‚’AIã«ä¾é ¼ã—ã¾ã™...")
                conversation_history.append(types.Content(role="user", parts=[types.Part(text=user_feedback)]))
                continue
        await execute_plan(client, model_name, system_instruction, approved_plan, conversation_history)
        print("--- AIé–‹ç™ºã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº† ---\n")
    except Exception as e:
        print(f"\n[AGENT_FATAL_ERROR] ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‡¦ç†ä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", file=sys.stderr)
# --- ã“ã“ã¾ã§å¤‰æ›´ãªã— ---

# ==============================================================================
#  ãƒ¢ãƒ¼ãƒ‰ã”ã¨ã®ãƒãƒ³ãƒ‰ãƒ© (ã“ã“ã‹ã‚‰ãŒãƒ¢ãƒ¼ãƒ‰åˆ¥ã®å‡¦ç†)
# ==============================================================================

async def handle_one_shot_mode(client: genai.Client, model_name: str, args, system_instruction: str | None):
    """ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰ã®å‡¦ç†ã‚’è¡Œã†"""
    user_initial_instruction = args.prompt or sys.stdin.read()
    await run_agent_session(client, model_name, system_instruction, user_initial_instruction)


async def handle_interactive_mode(client: genai.Client, model_name: str, args, system_instruction: str | None):
    """å¯¾è©±ãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†ã‚’è¡Œã†ã€‚é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ©Ÿèƒ½ã‚’ä¸¡ç«‹ã•ã›ã‚‹ã€‚"""
    print("--- Interactive Chat Mode ---")
    print("é€šå¸¸ã®ä¼šè©±ãŒã§ãã¾ã™ã€‚")
    print("'/dev' ã¾ãŸã¯ '/agent' ã§é–‹ç™ºã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹ã—ã¾ã™ã€‚(ä¾‹: /dev ãƒ­ã‚°ã‚¤ãƒ³æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ @app/views.py)")
    print("è¤‡æ•°è¡Œã®å…¥åŠ›ã¯ [Ctrl+S] ã§ç¢ºå®šã€‚çµ‚äº†ã¯ 'exit' ã¾ãŸã¯ 'quit'ã€‚")
    
    chat_history_list = load_chat_history()
    
    try:
        # ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ¼ãƒ‰ç”¨ã®AIè¨­å®šï¼ˆGoogleæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’æœ‰åŠ¹åŒ–ãªã©ï¼‰
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã¯ç‹¬ç«‹ã•ã›ã‚‹
        chat_config = types.GenerateContentConfig(
            tools=[types.Tool(google_search=types.GoogleSearch())],
            system_instruction=system_instruction # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸTARONI.mdãªã©ã‚’ãƒãƒ£ãƒƒãƒˆã®æ€§æ ¼ä»˜ã‘ã«ä½¿ã†
        )
        # æ–°SDKã§ã¯ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å±¥æ­´ã¯æ‰‹å‹•ã§ç®¡ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        
        session = PromptSession(completer=PathCompleter())
        bindings = KeyBindings()
        @bindings.add('c-s')
        def _(event): event.app.current_buffer.validate_and_handle()

        while True:
            try:
                user_input = await session.prompt_async("You: ", multiline=True, key_bindings=bindings)
                
                if user_input.strip().lower() in ["exit", "quit"]:
                    print("å¯¾è©±ã‚’çµ‚äº†ã—ã¾ã™ã€‚"); break
                
                # --- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ã¸ã®åˆ‡ã‚Šæ›¿ãˆã‚’åˆ¤å®š ---
                if user_input.strip().startswith(("/dev", "/agent")):
                    instruction = re.sub(r'^/(dev|agent)\s*', '', user_input, count=1)
                    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã™éš›ã¯ã€ãƒãƒ£ãƒƒãƒˆã®å±¥æ­´ã§ã¯ãªãã€ç‹¬ç«‹ã—ãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ã¨ã—ã¦å®Ÿè¡Œ
                    await run_agent_session(client, model_name, system_instruction, instruction)
                    print("--- Interactive Chat Mode ã«æˆ»ã‚Šã¾ã—ãŸ ---")
                    continue
                
                # --- ã“ã“ã‹ã‚‰ãŒé€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆå‡¦ç† ---
                processed_input = parse_and_read_files_from_prompt(user_input)
                if not processed_input.strip(): continue

                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
                user_content_dict = {"role": "user", "parts": [{"text": processed_input}]}
                chat_history_list.append(user_content_dict)
                
                print_formatted_text(FormattedText([('italic', '...AIã«å•ã„åˆã‚ã›ä¸­...')]))

                # æ–°SDKã§ã¯æ¯å›å±¥æ­´ã‚’æ¸¡ã—ã¦APIã‚’å‘¼ã¶
                response = await client.aio.models.generate_content(
                    model=model_name,
                    contents=chat_history_list,
                    config=chat_config
                )
                
                # AIã®å¿œç­”ã‚’è¡¨ç¤º
                final_text = await add_citations(response) # å¼•ç”¨ä»˜ä¸ã‚‚å¯èƒ½
                print_formatted_text(FormattedText([('bold cyan', 'AI:'), ('', f' {final_text}')]))

                # AIã®å¿œç­”ã‚’ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
                # response.candidates[0].content ã«ã¯AIã®å¿œç­”å…¨ä½“ãŒå«ã¾ã‚Œã¦ã„ã‚‹
                if response.candidates:
                    #chat_history_list.append(response.candidates[0].content)
                    #chat_history_list.append(types.Content(role="model", parts=[types.Part(text=final_text)]))
                    user_content_dict = {"role": "model", "parts": [{"text": final_text}]}
                    chat_history_list.append(user_content_dict)
                
            except KeyboardInterrupt:
                print("\nå¯¾è©±ã‚’çµ‚äº†ã—ã¾ã™ã€‚"); break
    finally:
        print("[INFO] ã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«æœ€çµ‚çš„ãªãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜ã—ã¾ã™ã€‚", file=sys.stderr)
        print(chat_history_list)
        print('chat_history_list')
        save_chat_history(chat_history_list)
